# -*- coding: utf-8 -*-
"""modulo_entrenamiento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x--JxHBM6X-RRVybvW37WIWOTwYHVPKv

# Modulo de entrenamiento

Autores: Samuel Suarez, Pablo Bright y Juan Pablo Guzman

---

## Importación de librerias y lectura de datos
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder # decodificador de etiquetas y normalización de datos
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import os
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, classification_report, confusion_matrix
from sklearn.model_selection import KFold, train_test_split
import tensorflow as tf


encoder = LabelEncoder()

"""Se le da al código los diferentes archivos, estos los acompañamos del inicio y final de la medición, para no tener en cuenta momentos que nos sobran, y el notebook se comunica con el módulo de limpieza que nos va a retornar los datos preparados para implementar modelos de inteligencia artificial"""

os.system("python3 modulo_limpieza.py https://raw.githubusercontent.com/pbright23/Estribot/main/data/data_0805/Location.csv https://raw.githubusercontent.com/pbright23/Estribot/main/data/data_0805/Orientation.csv '2023-08-05 19:44:30' '2023-08-05 19:59:00' '[4,5,3,4.83,3.9]' '[25,20,25,5,345]' archive_output.csv")
os.system("python3 modulo_limpieza.py https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Location.csv https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Orientation.csv '2023-10-01 18:55:00' '2023-10-01 19:15:00' '[5.61, 7.6, 6.3, 8.3, 9.0, 9.0, 2.3, 7.3, 4.9, 6.6, 5.6, 6.6]' '[7, 61, 59, 73, 86, 75, 90, 104, 106, 82, 89, 86]' archive_output_big1.csv")
os.system("python3 modulo_limpieza.py https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Location.csv https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Orientation.csv '2023-10-01 19:30:30' '2023-10-01 19:44:00' '[5.61, 7.6, 6.3, 8.3, 9.0, 9.0, 2.3, 7.3, 4.9, 6.6, 5.6, 6.6]' '[7, 61, 59, 73, 86, 75, 90, 104, 106, 82, 89, 86]' archive_output_big2.csv")
os.system("python3 modulo_limpieza.py https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Location.csv https://media.githubusercontent.com/media/pbright23/Estribot/main/data/data_1001/Orientation.csv '2023-10-01 19:55:30' '2023-10-01 20:05:00' '[5.61, 7.6, 6.3, 8.3, 9.0, 9.0, 2.3, 7.3, 4.9, 6.6, 5.6, 6.6]' '[7, 61, 59, 73, 86, 75, 90, 104, 106, 82, 89, 86]' archive_output_big3.csv")

file_tacks = pd.read_csv('archive_output.csv', header = 'infer', sep = ',')
file_tacks.info()

file_total1 = pd.read_csv('archive_output_big1.csv', header = 'infer', sep=',')
file_total2 = pd.read_csv('archive_output_big2.csv', header = 'infer', sep=',')
file_total3 = pd.read_csv('archive_output_big3.csv', header = 'infer', sep=',')

file_total1['target'] = ["REGULAR","BUENO","REGULAR","MALO","REGULAR","BUENO","REGULAR","MALO","REGULAR","REGULAR","BUENO","BUENO","MALO","BUENO","REGULAR","REGULAR","BUENO","MALO","REGULAR","MALO","BUENO","BUENO","BUENO","MALO","REGULAR","REGULAR"]
file_total2['target'] = ["REGULAR", "MALO", "REGULAR", "BUENO", "REGULAR", "MALO", "BUENO", "BUENO", "MALO", "MALO", "BUENO", "REGULAR", "REGULAR", "REGULAR", "MALO", "BUENO", "BUENO", "BUENO","BUENO"]
file_total3['target'] = ["REGULAR", "MALO", "BUENO", "BUENO", "REGULAR", "BUENO", "MALO", "REGULAR", "BUENO", "REGULAR", "BUENO","REGULAR"]

file_total = pd.concat([
    #file_total1,
    file_total2,
    file_total3
    ])

"""## Preparar datos para los modelos"""

file_total.reset_index(inplace = True)

file_tacks['target'] = [ "REGULAR","REGULAR","BUENO","BUENO","BUENO","BUENO","REGULAR","BUENO","MALO","MALO","REGULAR","BUENO","MALO","BUENO", "BUENO", "MALO", "BUENO" ]


file_tacks = pd.concat([file_tacks, file_total])
file_tacks.info()

file_tacks.drop(['index', 'first_time', 'last_time'], axis = 1, inplace = True)
file_tacks.reset_index(inplace = True)

def augment_with_noise(dataframe, noise_multiplier=0.01):
    """
    Augments a DataFrame by adding Gaussian noise to each column based on its standard deviation.

    Args:
        dataframe (pd.DataFrame): The input DataFrame to be augmented.
        noise_multiplier (float): Multiplier for the standard deviation to control the noise level.

    Returns:
        pd.DataFrame: Augmented DataFrame.
    """
    augmented_data = dataframe.copy()

    for column in augmented_data.columns:
        if column == 'target':
          augmented_data['target'] = dataframe['target']
        else:
          std_dev = augmented_data[column].std()
          noise = np.random.normal(0, std_dev * noise_multiplier, size=len(augmented_data))
          augmented_data[column] += noise
    return augmented_data

cant = 800
while len(file_tacks) < cant:
  noisy_data = augment_with_noise(file_tacks)
  file_tacks = pd.concat([noisy_data, file_tacks])

print(file_tacks['target'].unique())
encoder = LabelEncoder()
file_tacks['target'] = encoder.fit_transform(file_tacks['target'])
file_tacks['target'].unique()

X = file_tacks.drop(['target'], axis = 1)
y = file_tacks['target']
X.info()

X.drop(['index'], axis = 1, inplace = True)

"""## Entrenamiento de modelos"""

y_data = y.values.reshape(-1,1)
print(y_data)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
y_data_encoded = np.array(ct.fit_transform(y_data))
print(y_data_encoded)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state= 42)
X_train_ann, X_test_ann, y_train_ann, y_test_ann = train_test_split(X.values, y_data_encoded, test_size=0.3, random_state= 42)

forest = RandomForestClassifier(max_depth=3 ,n_estimators = 50, criterion = 'gini')
tree = DecisionTreeClassifier(max_depth = 5,criterion = 'gini', splitter='best')
ann = tf.keras.models.Sequential()

forest.fit(X_train, y_train)
tree.fit(X_train, y_train)

ann.add(tf.keras.layers.Dense(units=9, activation='sigmoid'))
ann.add(tf.keras.layers.Dense(units=7, activation='sigmoid'))
#ann.add(tf.keras.layers.Dense(units=27, activation='sigmoid'))
#ann.add(tf.keras.layers.Dense(units=243, activation='sigmoid'))
#ann.add(tf.keras.layers.Dense(units=6561, activation='sigmoid'))


#ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

ann.add(tf.keras.layers.Dense(units=3, activation='softmax'))

ann.compile(optimizer = "adam", loss = 'categorical_crossentropy', metrics = ['accuracy', tf.keras.metrics.Recall()])

ann.fit(X_train_ann, y_train_ann, batch_size = 2, epochs = 100,)

"""## Validación Cruzada"""

from sklearn.metrics import accuracy_score

forest_predict = forest.predict(X_test)
tree_predict = tree.predict(X_test)
ann_predict = ann.predict(X_test_ann)

target_names = ['REGULAR' 'BUENO' 'MALO']

print("forest",classification_report(y_true = y_test, y_pred = forest_predict))
print("tree",classification_report(y_true = y_test, y_pred = tree_predict))
y_pred_ann = np.argmax(ann_predict, axis=1)
y_test_ann_fix = np.argmax(y_test_ann, axis=1)
print("neuronal network ",accuracy_score(y_true = y_test_ann_fix, y_pred = y_pred_ann))

print(confusion_matrix(y_true = y_test, y_pred = forest_predict),'\n')
print(confusion_matrix(y_true = y_test, y_pred = tree_predict),'\n')

"""## Exportar modelos a objetos"""

import joblib as jb

jb.dump(forest, 'forest.pkl', compress = 9)
jb.dump(tree, 'tree.pkl', compress = 9)
jb.dump(ann, 'ann.pkl', compress = 9)

